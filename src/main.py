import logging

from src.app.utils import load_model, load_tokenizer
from src.data.nlp import tokenize

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


def main():
    """
    Represents the main loop for the application
    """

    model, tfidf_vectorizer = load_model()
    tokenizer, lemmatizer, stopwords, punctuation = load_tokenizer()

    # Main loop
    while True:
        value = input("Enter a text to analyze: ")

        tokens = (
            tokenize(
                value,
                True,
                True,
                tokenizer,
                stopwords,
                punctuation,
                lemmatizer,
            )
            .__str__()
            .replace("'", "")
            .replace("[", "")
            .replace("]", "")
            .replace(",", "")
            .replace("CLS ", "")
            .replace("SEP", "")
            .replace("  ", " ")
        )

        text_tfidf = tfidf_vectorizer.transform([tokens])

        predictions = model.predict(text_tfidf)

        print("Was the text generated by LLMs?", predictions[0])


if __name__ == "__main__":
    main()
